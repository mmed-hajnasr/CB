{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_chunk = \"\"\"\n",
    "Chères lectrices, chers lecteurs,\n",
    "Depuis sa fondation en 2001, l'ISI s'est constamment adapté aux évolutions des besoins des formations d'ingénieurs, notamment dans le domaine de l'informatique et de ses applications. Au cours de la dernière décennie, nous avons constaté une augmentation significative des demandes d'admission émanant d'étudiants souhaitant rejoindre nos programmes. Cette tendance positive témoigne de l'intérêt croissant des jeunes talents pour nos cursus.\n",
    "Soucieux d'innover et de répondre à une diversité de besoins en formation, l'ISI accorde une attention particulière à l'employabilité de ses diplômés et à l'égalité des chances dans l'accès à ses formations. Dans cette perspective, nous prévoyons d'intensifier nos efforts afin d'attirer des candidats de qualité et de sélectionner les meilleurs profils.\n",
    "Dans une politique de rayonnement, nous nous engageons à renforcer nos partenariats avec des acteurs nationaux et internationaux, qu'ils soient du milieu académique ou industriel. Nous incitons également notre conseil scientifique à établir des collaborations étroites avec nos partenaires socio-économiques.\n",
    "\n",
    "Conscients de l'importance du développement pédagogique de notre corps enseignant, nous veillons à leur fournir les ressources nécessaires, que ce soit sur le plan matériel, numérique ou financier. Par ailleurs, nous nous efforçons d'évaluer et d'améliorer le niveau de satisfaction des étudiants, des enseignants et du personnel dans toutes nos activités pédagogiques, culturelles et organisationnelles. Dans cette optique, nous avons lancé une campagne de sensibilisation pour promouvoir notre démarche qualité et nous conformer aux normes internationales, notamment en mettant en place un système de management pour les organismes d'éducation selon la norme ISO 21001.\n",
    "Malgré notre envergure, avec près de 1600 étudiants, nous assumons pleinement notre responsabilité envers tous nos étudiants et les parties prenantes en :\n",
    "- Offrant une formation adaptée aux exigences du marché de l'emploi et aux besoins spécifiques.\n",
    "- Respectant les droits de propriété intellectuelle et la vie privée, tout en intégrant nos responsabilités sociales et environnementales ainsi que les besoins spécifiques en matière de formation.\n",
    "- Proposant un cadre propice à une vie étudiante épanouissante sur les plans culturel, sportif et associatif.\n",
    "Au nom de tous mes collègues et de l'administration, je tiens à exprimer ma profonde gratitude envers toutes les personnes qui ont contribué, par leur temps et leurs idées, à rehausser le prestige de l'ISI à travers divers projets, que ce soit en matière d'organisation, par la mise en place d'un système SMOE ISO 21001, d'accréditation selon le label CTI-EUR-ACE ou encore par notre participation à des projets nationaux et internationaux.  Nous croyons fermement que notre institution progressera davantage en favorisant une gouvernance transparente et inclusive, qui renforce la coopération, la coordination et la communication.  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type BufferedReader is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m text_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf_file \u001b[38;5;129;01min\u001b[39;00m pdf_files:\n\u001b[0;32m---> 29\u001b[0m     (key,text) \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     text_dict[key] \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# text_dict[file_name] = [line.strip() for line in text_dict[file_name] if line.strip() != '']\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# for csv_file in csv_files:\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     with open(csv_file, 'r', encoding='utf-8') as file:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#         text_dict[{\"source\":csv_file }] = file.read()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 25\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[0;34m(pdf_file)\u001b[0m\n\u001b[1;32m     23\u001b[0m         context[key]\u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mmetadata[key];\n\u001b[1;32m     24\u001b[0m context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m file    \n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m,text)\n",
      "File \u001b[0;32m/nix/store/h723hb9m43lybmvfxkk6n7j4v664qy7b-python3-3.11.9/lib/python3.11/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m/nix/store/h723hb9m43lybmvfxkk6n7j4v664qy7b-python3-3.11.9/lib/python3.11/json/encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/nix/store/h723hb9m43lybmvfxkk6n7j4v664qy7b-python3-3.11.9/lib/python3.11/json/encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/h723hb9m43lybmvfxkk6n7j4v664qy7b-python3-3.11.9/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type BufferedReader is not JSON serializable"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "pdf_files = glob.glob(os.path.join(current_directory, 'data/*.pdf'))\n",
    "csv_files = glob.glob(os.path.join(current_directory, 'data/*.csv'))\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        text = \"\"\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "        context = {}\n",
    "        for key in reader.metadata:\n",
    "            if key in [\"Author\",\"CreationDate\",\"Title\"]:\n",
    "                context[key]= reader.metadata[key];\n",
    "        context[\"source\"]= file    \n",
    "        return (str(context),text)\n",
    "\n",
    "text_dict = {}\n",
    "for pdf_file in pdf_files:\n",
    "    (key,text) = extract_text_from_pdf(pdf_file)\n",
    "    text_dict[key] = text\n",
    "    # text_dict[file_name] = [line.strip() for line in text_dict[file_name] if line.strip() != '']\n",
    "# for csv_file in csv_files:\n",
    "#     with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "#         text_dict[{\"source\":csv_file }] = file.read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1725572"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "def remove_newlines_without_uppercase(text):\n",
    "    # Regular expression to match \\n followed by a non-uppercase letter or end of string\n",
    "    return re.sub(r'\\n(?![A-Z])', '', text)\n",
    "\n",
    "def chunkerize(raw):\n",
    "    # Convert each item in the list to a string\n",
    "    converted = [remove_newlines_without_uppercase(str(item)) for item in raw]\n",
    "\n",
    "    # Join all items with a double newline\n",
    "    text_to_chunk = \"\".join(converted)\n",
    "\n",
    "    # Define the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \".\",\n",
    "            \",\",\n",
    "            \"-\",\n",
    "            \" \",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Use the text splitter on the combined string\n",
    "    return text_splitter.split_text(text_to_chunk)\n",
    "\n",
    "for file_name in text_dict:\n",
    "    text_dict[file_name] = chunkerize(text_dict[file_name])\n",
    "\n",
    "open(\"output.txt\", \"w\").write(str(text_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
